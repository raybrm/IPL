{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ipl I317B Sécurité : labos\n",
    "## Semaine 1 : introduction et outils python\n",
    "\n",
    "### Partie 1 : manipuler du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1\n",
    "Pour ce premier exercice, nous allons manipuler un court extrait de logs d'un serveur web nginx.  \n",
    "Pour chaque ligne de celui-ci, extrayez:\n",
    "- l'adresse IP\n",
    "- le timestamp\n",
    "- le chemin demandé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = \"\"\"91.160.48.68 - - [3/Sep/2020:18:18:10 +0200] \"GET /?C=M;O=D HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\" \"-\"\n",
    "78.224.72.158 - - [3/Sep/2020:18:24:52 +0200] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0\" \"-\"\n",
    "192.241.199.4 - - [3/Sep/2020:19:11:15 +0200] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 zgrab/0.x\" \"-\"\n",
    "40.121.69.161 - - [3/Sep/2020:19:16:32 +0200] \"GET /.env HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"-\"\n",
    "40.121.69.161 - - [3/Sep/2020:19:16:32 +0200] \"POST / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\" \"-\"\n",
    "185.172.110.223 - - [3/Sep/2020:19:16:37 +0200] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:76.0) Gecko/20100101 Firefox/76.0\" \"-\"\n",
    "66.249.75.16 - - [3/Sep/2020:19:23:28 +0200] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.97 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\" \"-\"\n",
    "66.249.75.23 - - [3/Sep/2020:19:23:30 +0200] \"GET / HTTP/1.1\" 200 0 \"-\" \"Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.97 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\" \"-\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in logs.split(\"\\n\"):\n",
    "    pass # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2\n",
    "Avec les mêmes logs que l'exercice précédent, comptez et affichez les différents user-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie 2 : requests et BeautifulSoup\n",
    "\n",
    "Assurez vous que les paquets suivant soient bien installé :\n",
    "\n",
    "`pip3 install --user requests bs4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le packet requests nous permet de faire des requêtes web facilement, par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "ipl_req = requests.get(\"http://www.vinci.be/fr-be/ipl/Pages/Accueil.aspx\")\n",
    "\n",
    "if ipl_req.ok:\n",
    "    ipl_html = ipl_req.text\n",
    "else:\n",
    "    print(ipl_req.status_code)\n",
    "    print(ipl_req.reason)\n",
    "\n",
    "# Doc requests : http://docs.python-requests.org/en/master/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ensuite lire le code source de la page en affichant `ipl_html`.\n",
    "\n",
    "Parser du html avec des split ou des regex deviendrait vite infernal, c'est là qu'intervient la librairie BeautifulSoup qui nous permet de parcourir beaucoup plus simplement le dom notamment avec les fonctions `find` et `find_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n\\t\\r\\n            Paul-Lambin\\r\\n            \\r\\n            \\r\\n            Accueil\\r\\n            \\r\\n        \\r\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(ipl_html)\n",
    "\n",
    "# pour récupérer le contenu de la balise <title> :\n",
    "title = soup.find(\"title\").get_text()\n",
    "\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `find` et la fonction `find_all` vous renverons un object de type `tag` (balise) sur lequel vous pouvez faire :\n",
    "  * d'autres `find()` et `find_all()`\n",
    "  * `get_text()` pour récupérer tout le contenu textuel de la balise\n",
    "  * `[\"nom de l'attribut\"]` pour récupérer un attribut en particulier (exemple: `tag['href']`)\n",
    "  * ...\n",
    "\n",
    "( Doc BeautifulSoup4 : https://www.crummy.com/software/BeautifulSoup/bs4/doc/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1:\n",
    "Récupérez le contenue de la page suivante et affichez son titre proprement (sans les retours à la ligne) : http://www.vinci.be/fr-be/ipl/Pages/Accueil.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de faire des requêtes beaucoup plus poussée en ajoutant des arguments correspondant aux attributs des balises que nous cherchons, par exemple pour trouver les balises `<div class=\"footer\"></div>` :\n",
    "\n",
    "```\n",
    "soup.find_all(\"div\", class_=\"footer\")\n",
    "# NB: class est un mot clé réservé en python, d'où l'ajout du \"_\" dans BeautifulSoup\n",
    "```\n",
    "\n",
    "Ou encore pour trouver tous les liens vers la page facebook de l'ipl :\n",
    "\n",
    "`soup.find_all(\"a\", href=\"https://www.facebook.com/HEVINCI\")`\n",
    "\n",
    "BeautifoulSoup accepte également des regex compilée en argument, il est donc possible de récupérer la liste des url qui ne sont pas du domaines de \"vinci.be\" :\n",
    "\n",
    "```\n",
    "import re\n",
    "\n",
    "not_vinci = re.compile(\"^http(s?)://(?!www.vinci.be/).*$\")\n",
    "soup.find_all(\"a\", href=not_vinci)\n",
    "```\n",
    "\n",
    "Il est également possible d'utiliser une fonction arbitraire pour valider les liens. Par exemple, pour trouver tous les liens de moins de 30 caractères :\n",
    "\n",
    "```\n",
    "def less_than_30(tag):\n",
    "    return len(tag) < 30  # on retourne un booleen\n",
    "    \n",
    "# on passe notre fonction en paramètre sans l'executer :\n",
    "soup.find_all(\"a\", href=less_than_30)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 2:\n",
    "\n",
    "Utilisez des regex pour récupérer la liste des liens http (et donc pas https) sur la page :  \n",
    "https://www.vinci.be/fr-be/ipl/Pages/Accueil.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 3:\n",
    "\n",
    "Utilisez une fonction pour récupérer tous les liens qui ne finissent pas par \".aspx\" sur la page (rappel : en python \"abc\".endswith(\"c\") renvoit un booleen) :  \n",
    "https://www.vinci.be/fr-be/ipl/Pages/Accueil.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 4:\n",
    "Depuis le site du Centre Cybersecurité Belgique https://ccb.belgium.be/fr/actualités , récupérez la liste des articles récents et affichez leurs titres ainsi qu'un lien vers l'article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 5:\n",
    "La lib requests permet également d'effectuer de requête POST. Pour tester cette fonctionnalitée, créez un \"requestbin\" sur le site http://requestbin.net/ puis utilisez requests pour poster des données vers votre requestbin.\n",
    "\n",
    "> RequestBin gives you a URL that will collect requests made to it and let you inspect them in a human-friendly way.\n",
    "Use RequestBin to see what your HTTP client is sending or to inspect and debug webhook requests.\n",
    "\n",
    "\n",
    "Voir : https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request (ou la cheatsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 6:\n",
    "\n",
    "Maintenant que vous maîtrisez le get, le post et quelques bases de parsing web, \"connectez-vous\" au formulaire suivant avec les identifiants : `admin/admin`  \n",
    "http://labosecuipl.alwaysdata.net/20/s01/form/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 7:\n",
    "Vous avez certainement constaté sur votre requestbin que la librairie requests utilise par défaut un User-Agent explicite \"*je suis un lib python pour faire des requêtes*\".  \n",
    "Documentez-vous sur la manière de changer celui-ci dans requests et remplacez le dans votre précédente requête vers requestbin par quelque chose de plus discret, par exemple l'user-agent le plus commun du moment :  \n",
    "`Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36`\n",
    "\n",
    "source: https://techblog.willshouse.com/2012/01/03/most-common-user-agents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
